{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Analysis of Public Complaints - Pipeline Development\n",
    "\n",
    "This notebook develops the NLP pipeline for analyzing public complaint data including:\n",
    "- Sentiment Analysis\n",
    "- Complaint Categorization\n",
    "- Keyword Extraction\n",
    "- Word Cloud Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_complaint_data(file_path):\n",
    "    \"\"\"\n",
    "    Load complaint data from CSV or Excel file\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.endswith(('.xlsx', '.xls')):\n",
    "        df = pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please use CSV or Excel files.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis using TextBlob\n",
    "    Returns sentiment polarity (-1 to 1) and subjectivity (0 to 1)\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return 0, 0, 'neutral'\n",
    "    \n",
    "    blob = TextBlob(str(text))\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    \n",
    "    # Classify sentiment\n",
    "    if polarity > 0.1:\n",
    "        sentiment_label = 'positive'\n",
    "    elif polarity < -0.1:\n",
    "        sentiment_label = 'negative'\n",
    "    else:\n",
    "        sentiment_label = 'neutral'\n",
    "    \n",
    "    return polarity, subjectivity, sentiment_label\n",
    "\n",
    "def batch_sentiment_analysis(texts):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on a list of texts\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        polarity, subjectivity, label = analyze_sentiment(text)\n",
    "        results.append({\n",
    "            'polarity': polarity,\n",
    "            'subjectivity': subjectivity,\n",
    "            'sentiment': label\n",
    "        })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complaint Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_complaints(texts, n_categories=5):\n",
    "    \"\"\"\n",
    "    Categorize complaints using K-means clustering on TF-IDF vectors\n",
    "    \"\"\"\n",
    "    # Preprocess texts\n",
    "    processed_texts = [preprocess_text(text) for text in texts]\n",
    "    \n",
    "    # Remove empty texts\n",
    "    processed_texts = [text for text in processed_texts if text.strip()]\n",
    "    \n",
    "    if len(processed_texts) < n_categories:\n",
    "        n_categories = max(1, len(processed_texts))\n",
    "    \n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=100,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_categories, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "    \n",
    "    # Get top terms for each cluster\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    cluster_labels = []\n",
    "    \n",
    "    for i in range(n_categories):\n",
    "        top_indices = kmeans.cluster_centers_[i].argsort()[-3:][::-1]\n",
    "        top_terms = [feature_names[idx] for idx in top_indices]\n",
    "        cluster_labels.append(' '.join(top_terms))\n",
    "    \n",
    "    return clusters, cluster_labels, vectorizer, kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(texts, top_n=20):\n",
    "    \"\"\"\n",
    "    Extract top keywords using TF-IDF\n",
    "    \"\"\"\n",
    "    # Preprocess texts\n",
    "    processed_texts = [preprocess_text(text) for text in texts if text and not pd.isna(text)]\n",
    "    \n",
    "    if not processed_texts:\n",
    "        return []\n",
    "    \n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=top_n * 2,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n",
    "    \n",
    "    # Get feature names and scores\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "    \n",
    "    # Create keyword-score pairs\n",
    "    keyword_scores = list(zip(feature_names, tfidf_scores))\n",
    "    keyword_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return keyword_scores[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Cloud Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(texts, width=800, height=400):\n",
    "    \"\"\"\n",
    "    Generate word cloud from complaint texts\n",
    "    \"\"\"\n",
    "    # Combine all texts\n",
    "    combined_text = ' '.join([preprocess_text(text) for text in texts if text and not pd.isna(text)])\n",
    "    \n",
    "    if not combined_text.strip():\n",
    "        return None\n",
    "    \n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        background_color='white',\n",
    "        stopwords=set(['complaint', 'issue', 'problem', 'service', 'customer']),\n",
    "        max_words=100,\n",
    "        colormap='viridis'\n",
    "    ).generate(combined_text)\n",
    "    \n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete NLP Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_nlp_analysis(df, text_column):\n",
    "    \"\"\"\n",
    "    Perform complete NLP analysis on complaint data\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Get text data\n",
    "    texts = df[text_column].fillna('').tolist()\n",
    "    \n",
    "    # 1. Sentiment Analysis\n",
    "    print(\"Performing sentiment analysis...\")\n",
    "    sentiment_results = batch_sentiment_analysis(texts)\n",
    "    results['sentiment'] = sentiment_results\n",
    "    \n",
    "    # 2. Complaint Categorization\n",
    "    print(\"Categorizing complaints...\")\n",
    "    clusters, cluster_labels, vectorizer, kmeans = categorize_complaints(texts)\n",
    "    results['categories'] = {\n",
    "        'clusters': clusters,\n",
    "        'labels': cluster_labels,\n",
    "        'vectorizer': vectorizer,\n",
    "        'model': kmeans\n",
    "    }\n",
    "    \n",
    "    # 3. Keyword Extraction\n",
    "    print(\"Extracting keywords...\")\n",
    "    keywords = extract_keywords(texts)\n",
    "    results['keywords'] = keywords\n",
    "    \n",
    "    # 4. Word Cloud\n",
    "    print(\"Generating word cloud...\")\n",
    "    wordcloud = generate_wordcloud(texts)\n",
    "    results['wordcloud'] = wordcloud\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample complaint data for testing\n",
    "sample_complaints = [\n",
    "    \"The internet service is extremely slow and keeps disconnecting\",\n",
    "    \"Billing department charged me twice for the same service\",\n",
    "    \"Customer service representative was very rude and unhelpful\",\n",
    "    \"Water supply has been disrupted for three days without notice\",\n",
    "    \"Garbage collection has been irregular in our neighborhood\",\n",
    "    \"Road maintenance is poor, many potholes need fixing\",\n",
    "    \"Public transportation is always delayed and overcrowded\",\n",
    "    \"Hospital staff provided excellent care during emergency\",\n",
    "    \"Library services have improved significantly this year\",\n",
    "    \"Park maintenance team does a great job keeping it clean\"\n",
    "]\n",
    "\n",
    "# Create sample DataFrame\n",
    "sample_df = pd.DataFrame({\n",
    "    'complaint_id': range(1, len(sample_complaints) + 1),\n",
    "    'complaint_text': sample_complaints,\n",
    "    'date': pd.date_range('2024-01-01', periods=len(sample_complaints), freq='D')\n",
    "})\n",
    "\n",
    "print(\"Sample Data:\")\n",
    "print(sample_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete NLP pipeline\n",
    "results = complete_nlp_analysis(sample_df, 'complaint_text')\n",
    "\n",
    "print(\"\\n=== NLP Analysis Results ===\")\n",
    "print(f\"\\nSentiment Distribution:\")\n",
    "print(results['sentiment']['sentiment'].value_counts())\n",
    "\n",
    "print(f\"\\nTop Keywords:\")\n",
    "for keyword, score in results['keywords'][:10]:\n",
    "    print(f\"{keyword}: {score:.3f}\")\n",
    "\n",
    "print(f\"\\nComplaint Categories:\")\n",
    "for i, label in enumerate(results['categories']['labels']):\n",
    "    count = sum(1 for c in results['categories']['clusters'] if c == i)\n",
    "    print(f\"Category {i+1}: {label} ({count} complaints)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_counts = results['sentiment']['sentiment'].value_counts()\n",
    "axes[0, 0].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "axes[0, 0].set_title('Sentiment Distribution')\n",
    "\n",
    "# Sentiment polarity histogram\n",
    "axes[0, 1].hist(results['sentiment']['polarity'], bins=10, alpha=0.7)\n",
    "axes[0, 1].set_title('Sentiment Polarity Distribution')\n",
    "axes[0, 1].set_xlabel('Polarity')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Category distribution\n",
    "category_counts = pd.Series(results['categories']['clusters']).value_counts().sort_index()\n",
    "axes[1, 0].bar(range(len(category_counts)), category_counts.values)\n",
    "axes[1, 0].set_title('Complaint Categories')\n",
    "axes[1, 0].set_xlabel('Category')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# Top keywords\n",
    "top_keywords = results['keywords'][:8]\n",
    "keywords, scores = zip(*top_keywords)\n",
    "axes[1, 1].barh(range(len(keywords)), scores)\n",
    "axes[1, 1].set_yticks(range(len(keywords)))\n",
    "axes[1, 1].set_yticklabels(keywords)\n",
    "axes[1, 1].set_title('Top Keywords')\n",
    "axes[1, 1].set_xlabel('TF-IDF Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display word cloud\n",
    "if results['wordcloud']:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(results['wordcloud'], interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Complaint Word Cloud', fontsize=16)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No word cloud generated - insufficient text data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
